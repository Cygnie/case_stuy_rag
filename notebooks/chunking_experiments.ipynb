{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee69814",
   "metadata": {},
   "source": [
    "# RAG Pipeline Experiments\n",
    "\n",
    "This notebook documents the experimental process for building the NTT Data Agent System's RAG pipeline.\n",
    "\n",
    "**Experiments covered:**\n",
    "1. Chunking Strategies (Markdown vs Recursive)\n",
    "2. Vector Store Ingestion with Gemini Embeddings\n",
    "3. Agent Tool Development with Year Filtering\n",
    "4. Streaming Response Testing\n",
    "\n",
    "> **Note:** The final production code has been extracted to `src/` modules. This notebook serves as documentation of the R&D process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac99712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to path: c:\\Users\\ahmet\\OneDrive\\Masaüstü\\ntt_case\\ntt_clean_start\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(f\"Project root added to path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637adf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n",
      "Config: QDRANT_URL=http://localhost:6333, COLLECTION=ntt_semantic_search_v2\n"
     ]
    }
   ],
   "source": [
    "# Import chunking utilities - directly from libraries (experiment setup)\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tiktoken\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration (will be moved to src/utils/config.py after experiments)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "QDRANT_COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION_NAME\", \"ntt_semantic_search_v1\")\n",
    "EMBEDDING_MODEL = \"models/embedding-001\"\n",
    "LLM_MODEL = \"gemini-2.5-flash\"\n",
    "RAG_K = 5\n",
    "\n",
    "print(\"Modules imported successfully.\")\n",
    "print(f\"Config: QDRANT_URL={QDRANT_URL}, COLLECTION={QDRANT_COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d16d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 markdown files:\n",
      "- sr_2019_p.md\n",
      "- sr_2020.md\n",
      "- sr_2020_cb_p.md\n",
      "- sr_2021.md\n",
      "- sr_2020_cb_v.md\n",
      "- sr_2022.md\n",
      "- sr2023.md\n",
      "- sr_2023_cb_v.md\n",
      "- sr2024.md\n",
      "- sr_2024_cb_v.md\n"
     ]
    }
   ],
   "source": [
    "# Find all processed markdown files\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "md_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(processed_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".md\"):\n",
    "            md_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(md_files)} markdown files:\")\n",
    "for f in md_files:\n",
    "    print(f\"- {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7187ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment 1: Markdown Header Splitter\n",
    "\n",
    "This approach splits text based on markdown headers (#, ##, ###).\n",
    "Pros: Preserves document structure and semantic boundaries.\n",
    "Cons: Chunk sizes can be highly variable.\n",
    "\n",
    "Conclusion: Good for structured documents, but needs secondary splitting for large sections.\n",
    "\"\"\"\n",
    "print(f\"{'='*20} MARKDOWN CHUNKER ANALYSIS {'='*20}\")\n",
    "\n",
    "# Direct implementation using LangChain\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for file_path in md_files[:2]:  # Limit to first 2 files for demo\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        chunks = markdown_splitter.split_text(content)\n",
    "        tokens = [len(enc.encode(chunk.page_content)) for chunk in chunks]\n",
    "        \n",
    "        print(f\"\\nFile: {os.path.basename(file_path)}\")\n",
    "        print(f\"Total Chunks: {len(chunks)}\")\n",
    "        print(f\"Avg Tokens: {sum(tokens)/len(tokens):.0f}, Min: {min(tokens)}, Max: {max(tokens)}\")\n",
    "        \n",
    "        if len(chunks) > 0:\n",
    "             print(f\"Sample Chunk Metadata: {chunks[0].metadata}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tokens, bins=30, color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Markdown Header Splitter - {os.path.basename(file_path)}')\n",
    "        plt.xlabel('Token Count')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.axvline(x=1000, color='red', linestyle='--', label='Target: 1000 tokens')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6975bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample chunk with metadata\n",
    "if chunks:\n",
    "    print(\"Sample Chunk (index 4 or last available):\")\n",
    "    sample_idx = min(4, len(chunks) - 1)\n",
    "    print(f\"Metadata: {chunks[sample_idx].metadata}\")\n",
    "    print(f\"Content Preview: {chunks[sample_idx].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49bf83",
   "metadata": {},
   "source": [
    "# Recursive Character Splitter with Token Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f9a940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== RECURSIVE CHUNKER ANALYSIS ====================\n",
      "Processed sr_2019_p.md (Year: 2019): 98 chunks (filtered from 259)\n",
      "Processed sr_2020.md (Year: 2020): 80 chunks (filtered from 447)\n",
      "Processed sr_2020_cb_p.md (Year: 2020): 9 chunks (filtered from 11)\n",
      "Processed sr_2021.md (Year: 2021): 86 chunks (filtered from 426)\n",
      "Processed sr_2020_cb_v.md (Year: 2022): 18 chunks (filtered from 18)\n",
      "Processed sr_2022.md (Year: 2022): 90 chunks (filtered from 311)\n",
      "Processed sr2023.md (Year: 2023): 108 chunks (filtered from 602)\n",
      "Processed sr_2023_cb_v.md (Year: 2023): 16 chunks (filtered from 27)\n",
      "Processed sr2024.md (Year: 2024): 111 chunks (filtered from 475)\n",
      "Processed sr_2024_cb_v.md (Year: 2024): 34 chunks (filtered from 44)\n",
      "\n",
      "Total documents prepared: 650\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*20} RECURSIVE CHUNKER ANALYSIS {'='*20}\")\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from datetime import datetime\n",
    "\n",
    "# Chunking config - derived from this experiment\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 150\n",
    "MIN_CHUNK_SIZE = 300\n",
    "\n",
    "# Direct implementation using LangChain\n",
    "recursive_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "all_documents = []\n",
    "\n",
    "for file_path in md_files:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Extract Metadata from path\n",
    "        path_parts = os.path.normpath(file_path).split(os.sep)\n",
    "        year_folder = path_parts[-2]\n",
    "        year = year_folder.replace(\"ntt_\", \"\") if \"ntt_\" in year_folder else \"unknown\"\n",
    "        source = path_parts[-1]\n",
    "        \n",
    "        # Get file stats\n",
    "        file_stats = os.stat(file_path)\n",
    "        update_date = datetime.fromtimestamp(file_stats.st_mtime).isoformat()\n",
    "        \n",
    "        # Split text\n",
    "        raw_chunks = recursive_splitter.split_text(content)\n",
    "        \n",
    "        # Filter by minimum token count\n",
    "        valid_chunks = []\n",
    "        for i, chunk_text in enumerate(raw_chunks):\n",
    "            token_count = len(enc.encode(chunk_text))\n",
    "            if token_count >= MIN_CHUNK_SIZE:\n",
    "                doc = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\n",
    "                        \"source\": source,\n",
    "                        \"year\": int(year) if year.isdigit() else year,\n",
    "                        \"chunk_index\": len(valid_chunks),\n",
    "                        \"token_count\": token_count,\n",
    "                        \"update_date\": update_date,\n",
    "                    }\n",
    "                )\n",
    "                valid_chunks.append(doc)\n",
    "        \n",
    "        all_documents.extend(valid_chunks)\n",
    "        print(f\"Processed {source} (Year: {year}): {len(valid_chunks)} chunks (filtered from {len(raw_chunks)})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents prepared: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5d5a0",
   "metadata": {},
   "source": [
    "# Experiment 3: Indexing with FastEmbed & Qdrant (Cosine Similarity)\n",
    "\n",
    "This step indexes the chunks generated above into Qdrant using **FastEmbed** embeddings.\n",
    "We explicitly configure the collection to use **Cosine Similarity** and ensure metadata (like `year`) is preserved for filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588a2c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== INDEXING LIKE SRC/VECTORSTORE/INDEXER.PY (GEMINI DENSE) ====================\n",
      "Connecting to Qdrant at http://localhost:6333...\n",
      "Initializing Gemini Embeddings with model: models/embedding-001\n",
      "Detected vector size: 768\n",
      "Collection 'ntt_hybrid_experiment' created with Hybrid Config (Gemini Dense).\n",
      "Payload index for 'year' created.\n",
      "Indexing 650 documents...\n",
      "Indexed batch 1/65\n",
      "Indexed batch 2/65\n",
      "Indexed batch 3/65\n",
      "Indexed batch 4/65\n",
      "Indexed batch 5/65\n",
      "Indexed batch 6/65\n",
      "Indexed batch 7/65\n",
      "Indexed batch 8/65\n",
      "Indexed batch 9/65\n",
      "Indexed batch 10/65\n",
      "Indexed batch 11/65\n",
      "Indexed batch 12/65\n",
      "Indexed batch 13/65\n",
      "Indexed batch 14/65\n",
      "Indexed batch 15/65\n",
      "Indexed batch 16/65\n",
      "Indexed batch 17/65\n",
      "Indexed batch 18/65\n",
      "Indexed batch 19/65\n",
      "Indexed batch 20/65\n",
      "Indexed batch 21/65\n",
      "Indexed batch 22/65\n",
      "Indexed batch 23/65\n",
      "Indexed batch 24/65\n",
      "Indexed batch 25/65\n",
      "Indexed batch 26/65\n",
      "Indexed batch 27/65\n",
      "Indexed batch 28/65\n",
      "Indexed batch 29/65\n",
      "Indexed batch 30/65\n",
      "Indexed batch 31/65\n",
      "Indexed batch 32/65\n",
      "Indexed batch 33/65\n",
      "Indexed batch 34/65\n",
      "Indexed batch 35/65\n",
      "Indexed batch 36/65\n",
      "Indexed batch 37/65\n",
      "Indexed batch 38/65\n",
      "Indexed batch 39/65\n",
      "Indexed batch 40/65\n",
      "Indexed batch 41/65\n",
      "Indexed batch 42/65\n",
      "Indexed batch 43/65\n",
      "Indexed batch 44/65\n",
      "Indexed batch 45/65\n",
      "Indexed batch 46/65\n",
      "Indexed batch 47/65\n",
      "Indexed batch 48/65\n",
      "Indexed batch 49/65\n",
      "Indexed batch 50/65\n",
      "Indexed batch 51/65\n",
      "Indexed batch 52/65\n",
      "Indexed batch 53/65\n",
      "Indexed batch 54/65\n",
      "Indexed batch 55/65\n",
      "Indexed batch 56/65\n",
      "Indexed batch 57/65\n",
      "Indexed batch 58/65\n",
      "Indexed batch 59/65\n",
      "Indexed batch 60/65\n",
      "Indexed batch 61/65\n",
      "Indexed batch 62/65\n",
      "Indexed batch 63/65\n",
      "Indexed batch 64/65\n",
      "Indexed batch 65/65\n",
      "Indexing complete.\n",
      "\n",
      "Testing Hybrid Search with Year Filter (2023)...\n",
      "- [2023] | Reduce waste and create a society where the value of products and services continues to circulate ...\n",
      "- [2023] | Create new value through co-creation with various companies to achieve a smart and innovative soci...\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*20} INDEXING LIKE SRC/VECTORSTORE/INDEXER.PY (GEMINI DENSE) {'='*20}\")\n",
    "\n",
    "import uuid\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from fastembed import SparseTextEmbedding\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 1. Setup Clients & Models\n",
    "# Connect to real Qdrant instance\n",
    "print(f\"Connecting to Qdrant at {QDRANT_URL}...\")\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "COLLECTION_NAME = \"ntt_hybrid_experiment\"\n",
    "\n",
    "# Dense Embedding (Gemini)\n",
    "print(f\"Initializing Gemini Embeddings with model: {EMBEDDING_MODEL}\")\n",
    "# User requested task_type=\"SEMANTIC_SIMILARITY\"\n",
    "dense_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    task_type=\"semantic_similarity\"\n",
    ")\n",
    "\n",
    "# Get dimension dynamically\n",
    "dummy_vector = dense_model.embed_query(\"test\")\n",
    "vector_size = len(dummy_vector)\n",
    "print(f\"Detected vector size: {vector_size}\")\n",
    "\n",
    "# Sparse Embedding (FastEmbed - BM25)\n",
    "sparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# 2. Create Collection (Hybrid: Dense + Sparse)\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=vector_size, # Dynamic size\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,\n",
    "            )\n",
    "        )\n",
    "    }\n",
    ")\n",
    "print(f\"Collection '{COLLECTION_NAME}' created with Hybrid Config (Gemini Dense).\")\n",
    "\n",
    "# 3. Create Payload Index\n",
    "client.create_payload_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    field_name=\"year\",\n",
    "    field_schema=models.PayloadSchemaType.INTEGER\n",
    ")\n",
    "print(\"Payload index for 'year' created.\")\n",
    "\n",
    "# 4. Index Documents\n",
    "if 'all_documents' in locals() and all_documents:\n",
    "    print(f\"Indexing {len(all_documents)} documents...\")\n",
    "    \n",
    "    batch_size = 10\n",
    "    total_docs = len(all_documents)\n",
    "    \n",
    "    for i in range(0, total_docs, batch_size):\n",
    "        batch_docs = all_documents[i : i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch_docs]\n",
    "        metadatas = [doc.metadata for doc in batch_docs]\n",
    "        \n",
    "        # Generate Embeddings\n",
    "        # Dense (Gemini)\n",
    "        dense_embeddings = dense_model.embed_documents(texts)\n",
    "        \n",
    "        # Sparse (FastEmbed)\n",
    "        sparse_embeddings = list(sparse_model.embed(texts))\n",
    "        \n",
    "        points = []\n",
    "        for j, (text, meta, dense, sparse) in enumerate(zip(texts, metadatas, dense_embeddings, sparse_embeddings)):\n",
    "            # Create PointStruct\n",
    "            points.append(models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"dense\": dense, # Gemini returns list[float] directly\n",
    "                    \"sparse\": models.SparseVector(\n",
    "                        indices=sparse.indices.tolist(),\n",
    "                        values=sparse.values.tolist()\n",
    "                    )\n",
    "                },\n",
    "                payload={\n",
    "                    \"content\": text, # Mapped to 'content' as requested\n",
    "                    **meta\n",
    "                }\n",
    "            ))\n",
    "            \n",
    "        # Upsert Batch\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points\n",
    "        )\n",
    "        print(f\"Indexed batch {i//batch_size + 1}/{(total_docs + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "    print(\"Indexing complete.\")\n",
    "\n",
    "# 5. Verify Search (Hybrid + Filter)\n",
    "print(\"\\nTesting Hybrid Search with Year Filter (2023)...\")\n",
    "query_text = \"sustainability goals\"\n",
    "\n",
    "# Embed Query\n",
    "query_dense = dense_model.embed_query(query_text)\n",
    "query_sparse = list(sparse_model.embed([query_text]))[0]\n",
    "\n",
    "results = client.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    prefetch=[\n",
    "        models.Prefetch(\n",
    "            query=query_dense,\n",
    "            using=\"dense\",\n",
    "            filter=models.Filter(\n",
    "                must=[models.FieldCondition(key=\"year\", match=models.MatchValue(value=2023))]\n",
    "            ),\n",
    "            limit=2\n",
    "        ),\n",
    "        models.Prefetch(\n",
    "            query=models.SparseVector(indices=query_sparse.indices.tolist(), values=query_sparse.values.tolist()),\n",
    "            using=\"sparse\",\n",
    "            filter=models.Filter(\n",
    "                must=[models.FieldCondition(key=\"year\", match=models.MatchValue(value=2023))]\n",
    "            ),\n",
    "            limit=2\n",
    "        ),\n",
    "    ],\n",
    "    query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "for point in results.points:\n",
    "    print(f\"- [{point.payload.get('year')}] {point.payload.get('content')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69f879",
   "metadata": {},
   "source": [
    "# Agent Tool with Year Filtering (Initial Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== AGENT TOOL EXPERIMENT (v1 - Single Year) ====================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_qdrant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_agent\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_qdrant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantVectorStore\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_qdrant'"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*20} AGENT TOOL EXPERIMENT (v1 - Single Year) {'='*20}\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from pydantic import BaseModel, Field\n",
    "from qdrant_client.http import models\n",
    "from typing import Optional\n",
    "\n",
    "# Initialize embeddings directly\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize Qdrant client directly\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "# Use the same collection name as defined in the indexing cell\n",
    "COLLECTION_NAME = \"ntt_hybrid_experiment\" \n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "print(f\"Connected to Qdrant collection: {COLLECTION_NAME}\")\n",
    "\n",
    "# Initial schema - single year (later improved to List[int])\n",
    "class SearchInputV1(BaseModel):\n",
    "    query: str = Field(description=\"The semantic search query.\")\n",
    "    year: Optional[str] = Field(default=None, description=\"Single year to filter by.\")\n",
    "\n",
    "@tool(args_schema=SearchInputV1)\n",
    "def search_knowledge_base_v1(query: str, year: Optional[str] = None) -> str:\n",
    "    \"\"\"Initial version - single year filter. See v2 for improved version.\"\"\"\n",
    "    try:\n",
    "        if year and year.isdigit():\n",
    "            year_int = int(year)\n",
    "            results = vector_store.similarity_search(\n",
    "                query=query,\n",
    "                k=3,\n",
    "                filter=models.Filter(\n",
    "                    must=[\n",
    "                        models.FieldCondition(\n",
    "                            key=\"metadata.year\", \n",
    "                            match=models.MatchValue(value=year_int)\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            results = vector_store.similarity_search(query, k=3)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No documents found.\"\n",
    "        \n",
    "        context = \"\"\n",
    "        for doc in results:\n",
    "            context += f\"Source: {doc.metadata.get('source')} (Year: {doc.metadata.get('year')})\\n\"\n",
    "            context += f\"Content: {doc.page_content[:500]}...\\n\\n\"\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"Tool v1 defined. Issue: Cannot handle year ranges like '2021-2024'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeaba35",
   "metadata": {},
   "source": [
    "## Experiment 4: Streaming Agent with Year List Filter\n",
    "\n",
    "**Improvement:** Changed `year: str` to `years: List[int]` to support range queries.\n",
    "\n",
    "This is the approach used in production (`src/agent/tools/rag_tool.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e35b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*20} AGENT TOOL EXPERIMENT (v2 - Year List + Streaming) {'='*20}\")\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "# Improved schema - List[int] for year ranges\n",
    "class SearchInputV2(BaseModel):\n",
    "    query: str = Field(description=\"The semantic search query to find relevant information.\")\n",
    "    years: Optional[List[int]] = Field(\n",
    "        default=None, \n",
    "        description=\"List of specific years to filter by. If a range is given (e.g., '2021-2023'), expand it to a list: [2021, 2022, 2023].\"\n",
    "    )\n",
    "\n",
    "@tool(args_schema=SearchInputV2)\n",
    "def search_knowledge_base_v2(query: str, years: Optional[List[int]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Production-ready search tool.\n",
    "    Searches NTT Data sustainability reports with optional year filtering.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_filter = None\n",
    "        \n",
    "        if years and len(years) > 0:\n",
    "            # Use MatchAny for efficient OR filtering\n",
    "            search_filter = models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"metadata.year\", \n",
    "                        match=models.MatchAny(any=years)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        results = vector_store.similarity_search(\n",
    "            query=query,\n",
    "            k=RAG_K,\n",
    "            filter=search_filter\n",
    "        )\n",
    "\n",
    "        print(f\"[DEBUG] Query: '{query}', Years: {years}, Results: {len(results)}\")\n",
    "        for doc in results:\n",
    "            print(f\"  - Year: {doc.metadata.get('year')}, Source: {doc.metadata.get('source')}\")\n",
    "\n",
    "        if not results:\n",
    "            years_str = \", \".join(map(str, years)) if years else \"all years\"\n",
    "            return f\"No documents found for: {years_str}.\"\n",
    "        \n",
    "        context = \"\"\n",
    "        for doc in results:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            doc_year = doc.metadata.get('year', 'Unknown')\n",
    "            context += f\"Source: {source} (Year: {doc_year})\\n\"\n",
    "            context += f\"Content: {doc.page_content}\\n\"\n",
    "            context += \"-\" * 20 + \"\\n\"\n",
    "            \n",
    "        return context\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "# System prompt - derived from this experiment\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant for NTT Data. You have access to a tool that retrieves context from sustainability reports. \"\n",
    "    \"Use the tool to help answer user queries. \"\n",
    "    \"CRITICAL INSTRUCTION FOR DATES: \"\n",
    "    \"If the user asks for a range of years (e.g., 'between 2021 and 2023' or 'from 2020 to 2022'), \"\n",
    "    \"you MUST calculate all intermediate years and pass them as a list to the tool (e.g., years=[2021, 2022, 2023]).\"\n",
    ")\n",
    "\n",
    "# Create Agent directly\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=LLM_MODEL,\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_agent(llm, [search_knowledge_base_v2], system_prompt=SYSTEM_PROMPT)\n",
    "print(f\"Agent created with model: {LLM_MODEL}\")\n",
    "\n",
    "# Test with streaming\n",
    "print(f\"\\n{'='*20} STREAMING TEST {'='*20}\")\n",
    "user_input = \"2021-2024 arasında NTT Data'nın karbon nötr hedefleri nelerdir?\"\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
